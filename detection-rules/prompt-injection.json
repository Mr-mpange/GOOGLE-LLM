{
  "name": "Prompt Injection Detection",
  "type": "log_detection",
  "isEnabled": true,
  "queries": [
    {
      "query": "source:llm-guardian tags:security:prompt-injection",
      "aggregation": "count",
      "groupByFields": ["user", "session"],
      "distinctFields": [],
      "metric": "llm.security.injection_blocked"
    }
  ],
  "cases": [
    {
      "status": "high",
      "condition": "a > 0",
      "name": "Prompt injection attempt detected",
      "notifications": [
        "@slack-security-alerts",
        "@pagerduty-security"
      ]
    }
  ],
  "options": {
    "evaluationWindow": 300,
    "keepAlive": 3600,
    "maxSignalDuration": 86400,
    "detectionMethod": "threshold"
  },
  "message": "## Prompt Injection Detected\n\n**Severity:** High\n\n**Description:** A potential prompt injection attempt has been detected and blocked.\n\n**User:** {{user}}\n**Session:** {{session}}\n**Patterns Detected:** {{patterns}}\n\n**Action Required:**\n1. Review the blocked prompt in Datadog logs\n2. Investigate user activity history\n3. Consider blocking the user if repeated attempts\n4. Update injection detection patterns if needed\n\n**Logs:** [View in Datadog](https://app.datadoghq.com/logs?query=request_id:{{request_id}})\n\n**Runbook:** https://docs.company.com/security/prompt-injection-response",
  "tags": [
    "security:prompt-injection",
    "service:llm-guardian",
    "team:security"
  ]
}
